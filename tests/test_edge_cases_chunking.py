import os
from pathlib import Path
from src.processors.markdown_processor import MarkdownProcessor


def test_massive_document():
    """æµ‹è¯•è¶…å¤§æ–‡æ¡£çš„åˆ†å—"""
    processor = MarkdownProcessor()
    
    # åˆ›å»ºä¸€ä¸ªè¶…å¤§çš„æ–‡æ¡£ï¼ˆè¶…è¿‡100KBï¼‰
    massive_content = ""
    
    # æ·»åŠ å¤§é‡å†…å®¹
    for i in range(100):
        massive_content += f"""# ç« èŠ‚ {i}

è¿™æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„ç« èŠ‚ï¼ŒåŒ…å«å¤§é‡çš„å†…å®¹ã€‚

## å­ç« èŠ‚ {i}.1

è¿™é‡Œæœ‰å¾ˆå¤šå†…å®¹ï¼Œç”¨æ¥æµ‹è¯•åˆ†å—ç®—æ³•åœ¨å¤„ç†å¤§æ–‡æ¡£æ—¶çš„è¡¨ç°ã€‚

### å­å­ç« èŠ‚ {i}.1.1

æ›´å¤šçš„å†…å®¹...

### å­å­ç« èŠ‚ {i}.1.2

æ›´å¤šçš„å†…å®¹...

## å­ç« èŠ‚ {i}.2

è¿™é‡Œä¹Ÿæœ‰å¾ˆå¤šå†…å®¹...

"""
        
        # æ·»åŠ å¤§é‡æ–‡æœ¬å†…å®¹
        for j in range(50):
            massive_content += f"è¿™æ˜¯ç¬¬{i}ç« èŠ‚ç¬¬{j}æ®µçš„å†…å®¹ï¼ŒåŒ…å«å¾ˆå¤šæ–‡å­—ã€‚" * 10 + "\n\n"
    
    print(f"ğŸ“ è¶…å¤§æ–‡æ¡£æµ‹è¯•:")
    print(f"   æ–‡æ¡£å¤§å°: {len(massive_content):,} å­—ç¬¦")
    print(f"   é¢„æœŸåˆ†å—æ•°: çº¦ {len(massive_content) // 1000} ä¸ª")
    
    # æµ‹è¯•åˆ†å—
    chunks = processor.split_into_chunks(massive_content, chunk_size=1000, chunk_overlap=200)
    
    print(f"   å®é™…åˆ†å—æ•°: {len(chunks)}")
    print(f"   å¹³å‡åˆ†å—å¤§å°: {sum(len(c) for c in chunks) / len(chunks):.0f} å­—ç¬¦")
    
    # éªŒè¯åˆ†å—è´¨é‡
    assert len(chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    assert all(len(c) <= 1500 for c in chunks), "æ‰€æœ‰åˆ†å—éƒ½åº”è¯¥åœ¨åˆç†å¤§å°èŒƒå›´å†…"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†å—è¿‡å¤§
    oversized_chunks = [c for c in chunks if len(c) > 1200]
    print(f"   è¿‡å¤§åˆ†å—æ•°: {len(oversized_chunks)}")
    
    print("âœ… è¶…å¤§æ–‡æ¡£æµ‹è¯•é€šè¿‡ï¼")


def test_no_headers_document():
    """æµ‹è¯•æ²¡æœ‰æ ‡é¢˜çš„æ–‡æ¡£"""
    processor = MarkdownProcessor()
    
    # åˆ›å»ºä¸€ä¸ªæ²¡æœ‰æ ‡é¢˜çš„æ–‡æ¡£
    no_headers_content = """è¿™æ˜¯ä¸€ä¸ªæ²¡æœ‰æ ‡é¢˜çš„æ–‡æ¡£ã€‚

è¿™é‡Œåªæœ‰æ™®é€šçš„æ®µè½å†…å®¹ã€‚

æ›´å¤šçš„å†…å®¹...

ä»£ç ç¤ºä¾‹ï¼š
```python
def hello():
    print("Hello World")
```

æ›´å¤šçš„æ–‡æœ¬å†…å®¹...
""" * 100  # é‡å¤100æ¬¡
    
    print(f"ğŸ“ æ— æ ‡é¢˜æ–‡æ¡£æµ‹è¯•:")
    print(f"   æ–‡æ¡£å¤§å°: {len(no_headers_content):,} å­—ç¬¦")
    
    # æµ‹è¯•åˆ†å—
    chunks = processor.split_into_chunks(no_headers_content, chunk_size=500, chunk_overlap=100)
    
    print(f"   åˆ†å—æ•°: {len(chunks)}")
    print(f"   å¹³å‡åˆ†å—å¤§å°: {sum(len(c) for c in chunks) / len(chunks):.0f} å­—ç¬¦")
    
    # éªŒè¯åˆ†å—è´¨é‡
    assert len(chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    assert all(len(c) <= 750 for c in chunks), "æ‰€æœ‰åˆ†å—éƒ½åº”è¯¥åœ¨åˆç†å¤§å°èŒƒå›´å†…"
    
    # æ£€æŸ¥æ˜¯å¦å›é€€åˆ°ç®€å•åˆ†å—
    chunks_with_headers = [c for c in chunks if '#' in c]
    print(f"   åŒ…å«æ ‡é¢˜çš„åˆ†å—æ•°: {len(chunks_with_headers)}")
    
    print("âœ… æ— æ ‡é¢˜æ–‡æ¡£æµ‹è¯•é€šè¿‡ï¼")


def test_complex_nested_headers():
    """æµ‹è¯•å¤æ‚çš„åµŒå¥—æ ‡é¢˜ç»“æ„"""
    processor = MarkdownProcessor()
    
    # åˆ›å»ºä¸€ä¸ªæœ‰å¤æ‚åµŒå¥—æ ‡é¢˜çš„æ–‡æ¡£
    complex_content = """# ä¸»æ ‡é¢˜

## äºŒçº§æ ‡é¢˜1

### ä¸‰çº§æ ‡é¢˜1.1

#### å››çº§æ ‡é¢˜1.1.1

##### äº”çº§æ ‡é¢˜1.1.1.1

###### å…­çº§æ ‡é¢˜1.1.1.1.1

å†…å®¹...

#### å››çº§æ ‡é¢˜1.1.2

å†…å®¹...

### ä¸‰çº§æ ‡é¢˜1.2

å†…å®¹...

## äºŒçº§æ ‡é¢˜2

### ä¸‰çº§æ ‡é¢˜2.1

#### å››çº§æ ‡é¢˜2.1.1

å†…å®¹...

### ä¸‰çº§æ ‡é¢˜2.2

å†…å®¹...

# å¦ä¸€ä¸ªä¸»æ ‡é¢˜

## æ–°çš„äºŒçº§æ ‡é¢˜

å†…å®¹...
"""
    
    print(f"ğŸ—ï¸  å¤æ‚åµŒå¥—æ ‡é¢˜æµ‹è¯•:")
    print(f"   æ–‡æ¡£å¤§å°: {len(complex_content):,} å­—ç¬¦")
    
    # æµ‹è¯•åˆ†å—
    chunks = processor.split_into_chunks(complex_content, chunk_size=200, chunk_overlap=50)
    
    print(f"   åˆ†å—æ•°: {len(chunks)}")
    
    # åˆ†ææ¯ä¸ªåˆ†å—çš„æ ‡é¢˜å±‚çº§
    for i, chunk in enumerate(chunks, 1):
        lines = chunk.split('\n')
        headers = []
        for line in lines:
            if line.strip().startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                text = line.strip('#').strip()
                headers.append((level, text))
        
        print(f"   åˆ†å— {i}:")
        print(f"     å¤§å°: {len(chunk)} å­—ç¬¦")
        print(f"     æ ‡é¢˜å±‚çº§: {[f'H{h[0]}:{h[1][:20]}' for h in headers]}")
    
    # éªŒè¯åˆ†å—è´¨é‡
    assert len(chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    
    print("âœ… å¤æ‚åµŒå¥—æ ‡é¢˜æµ‹è¯•é€šè¿‡ï¼")


def test_mixed_content():
    """æµ‹è¯•æ··åˆå†…å®¹ï¼ˆä»£ç å—ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ï¼‰"""
    processor = MarkdownProcessor()
    
    # åˆ›å»ºä¸€ä¸ªåŒ…å«å„ç§ Markdown å…ƒç´ çš„æ–‡æ¡£
    mixed_content = """# æ··åˆå†…å®¹æµ‹è¯•

## ä»£ç å—

```hulo
fn hello() {
    echo "Hello World"
}
```

## åˆ—è¡¨

- é¡¹ç›®1
- é¡¹ç›®2
  - å­é¡¹ç›®2.1
  - å­é¡¹ç›®2.2
- é¡¹ç›®3

## è¡¨æ ¼

| åˆ—1 | åˆ—2 | åˆ—3 |
|-----|-----|-----|
| æ•°æ®1 | æ•°æ®2 | æ•°æ®3 |
| æ•°æ®4 | æ•°æ®5 | æ•°æ®6 |

## å¼•ç”¨

> è¿™æ˜¯ä¸€ä¸ªå¼•ç”¨å—
> åŒ…å«å¤šè¡Œå†…å®¹

## å¼ºè°ƒ

**ç²—ä½“æ–‡æœ¬** å’Œ *æ–œä½“æ–‡æœ¬*

## é“¾æ¥å’Œå›¾ç‰‡

[é“¾æ¥æ–‡æœ¬](https://example.com)

![å›¾ç‰‡æè¿°](image.jpg)

## æ›´å¤šå†…å®¹

è¿™é‡Œæœ‰å¾ˆå¤šæ™®é€šæ–‡æœ¬å†…å®¹...

""" * 20  # é‡å¤20æ¬¡
    
    print(f"ğŸ”€ æ··åˆå†…å®¹æµ‹è¯•:")
    print(f"   æ–‡æ¡£å¤§å°: {len(mixed_content):,} å­—ç¬¦")
    
    # æµ‹è¯•åˆ†å—
    chunks = processor.split_into_chunks(mixed_content, chunk_size=800, chunk_overlap=150)
    
    print(f"   åˆ†å—æ•°: {len(chunks)}")
    
    # æ£€æŸ¥åˆ†å—æ˜¯å¦ä¿æŒäº† Markdown ç»“æ„
    for i, chunk in enumerate(chunks[:3], 1):  # åªæ£€æŸ¥å‰3ä¸ªåˆ†å—
        print(f"   åˆ†å— {i}:")
        print(f"     å¤§å°: {len(chunk)} å­—ç¬¦")
        print(f"     åŒ…å«ä»£ç å—: {'```' in chunk}")
        print(f"     åŒ…å«åˆ—è¡¨: {'-' in chunk or '*' in chunk}")
        print(f"     åŒ…å«è¡¨æ ¼: {'|' in chunk}")
        print(f"     åŒ…å«å¼•ç”¨: {'>' in chunk}")
    
    # éªŒè¯åˆ†å—è´¨é‡
    assert len(chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    
    print("âœ… æ··åˆå†…å®¹æµ‹è¯•é€šè¿‡ï¼")


def test_extreme_edge_cases():
    """æµ‹è¯•æç«¯è¾¹ç•Œæƒ…å†µ"""
    processor = MarkdownProcessor()
    
    # æµ‹è¯•ç©ºæ–‡æ¡£
    empty_chunks = processor.split_into_chunks("")
    print(f"ğŸ“­ ç©ºæ–‡æ¡£æµ‹è¯•: {len(empty_chunks)} ä¸ªåˆ†å—")
    assert len(empty_chunks) == 0 or len(empty_chunks[0]) == 0, "ç©ºæ–‡æ¡£åº”è¯¥äº§ç”Ÿç©ºåˆ†å—æˆ–0ä¸ªåˆ†å—"
    
    # æµ‹è¯•åªæœ‰æ ‡é¢˜çš„æ–‡æ¡£
    headers_only = "# æ ‡é¢˜1\n## æ ‡é¢˜2\n### æ ‡é¢˜3"
    header_chunks = processor.split_into_chunks(headers_only)
    print(f"ğŸ“‹ åªæœ‰æ ‡é¢˜çš„æ–‡æ¡£: {len(header_chunks)} ä¸ªåˆ†å—")
    assert len(header_chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    
    # æµ‹è¯•è¶…å¤§æ ‡é¢˜
    huge_header = "# " + "éå¸¸å¤§çš„æ ‡é¢˜" * 1000
    huge_chunks = processor.split_into_chunks(huge_header)
    print(f"ğŸ“ è¶…å¤§æ ‡é¢˜: {len(huge_chunks)} ä¸ªåˆ†å—")
    assert len(huge_chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    
    # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦
    special_chars = "# ç‰¹æ®Šå­—ç¬¦æµ‹è¯•\n\nåŒ…å« `ä»£ç ` å’Œ **ç²—ä½“** å’Œ [é“¾æ¥](url) çš„å†…å®¹"
    special_chunks = processor.split_into_chunks(special_chars)
    print(f"ğŸ”¤ ç‰¹æ®Šå­—ç¬¦: {len(special_chunks)} ä¸ªåˆ†å—")
    assert len(special_chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    
    print("âœ… æç«¯è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è¾¹ç•Œæƒ…å†µå‹åŠ›æµ‹è¯•...\n")
    
    test_massive_document()
    print()
    
    test_no_headers_document()
    print()
    
    test_complex_nested_headers()
    print()
    
    test_mixed_content()
    print()
    
    test_extreme_edge_cases()
    print()
    
    print("ğŸ‰ æ‰€æœ‰è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆï¼") 