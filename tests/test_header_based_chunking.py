import os
from pathlib import Path
from src.processors.markdown_processor import MarkdownProcessor


def test_header_based_chunking():
    """æµ‹è¯•åŸºäºæ ‡é¢˜çš„åˆ†å—ç­–ç•¥"""
    processor = MarkdownProcessor()
    
    # åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„ Markdown å†…å®¹
    test_content = """# ä¸»æ ‡é¢˜

è¿™æ˜¯ä¸»æ ‡é¢˜ä¸‹çš„å†…å®¹ã€‚

## äºŒçº§æ ‡é¢˜1

è¿™æ˜¯äºŒçº§æ ‡é¢˜1çš„å†…å®¹ã€‚

### ä¸‰çº§æ ‡é¢˜1.1

è¿™æ˜¯ä¸‰çº§æ ‡é¢˜1.1çš„å†…å®¹ã€‚

### ä¸‰çº§æ ‡é¢˜1.2

è¿™æ˜¯ä¸‰çº§æ ‡é¢˜1.2çš„å†…å®¹ã€‚

## äºŒçº§æ ‡é¢˜2

è¿™æ˜¯äºŒçº§æ ‡é¢˜2çš„å†…å®¹ã€‚

### ä¸‰çº§æ ‡é¢˜2.1

è¿™æ˜¯ä¸‰çº§æ ‡é¢˜2.1çš„å†…å®¹ã€‚

# å¦ä¸€ä¸ªä¸»æ ‡é¢˜

è¿™æ˜¯å¦ä¸€ä¸ªä¸»æ ‡é¢˜çš„å†…å®¹ã€‚
"""
    
    # æµ‹è¯•åˆ†å—
    chunks = processor.split_into_chunks(test_content, chunk_size=200, chunk_overlap=50)
    
    print("ğŸ“ åŸºäºæ ‡é¢˜çš„åˆ†å—æµ‹è¯•:")
    print(f"   åŸå§‹å†…å®¹é•¿åº¦: {len(test_content)} å­—ç¬¦")
    print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
    print()
    
    for i, chunk in enumerate(chunks, 1):
        print(f"   åˆ†å— {i}:")
        print(f"     é•¿åº¦: {len(chunk)} å­—ç¬¦")
        print(f"     å†…å®¹é¢„è§ˆ: {chunk[:100]}...")
        print(f"     æ˜¯å¦åŒ…å«æ ‡é¢˜: {'æ˜¯' if '#' in chunk else 'å¦'}")
        print()
    
    # éªŒè¯åˆ†å—è´¨é‡
    assert len(chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    
    # æ£€æŸ¥æ¯ä¸ªåˆ†å—æ˜¯å¦åŒ…å«æ ‡é¢˜
    chunks_with_headers = [c for c in chunks if '#' in c]
    print(f"   åŒ…å«æ ‡é¢˜çš„åˆ†å—æ•°: {len(chunks_with_headers)}/{len(chunks)}")
    
    # æ£€æŸ¥åˆ†å—å¤§å°æ˜¯å¦åˆç†
    chunk_sizes = [len(c) for c in chunks]
    avg_size = sum(chunk_sizes) / len(chunk_sizes)
    print(f"   å¹³å‡åˆ†å—å¤§å°: {avg_size:.0f} å­—ç¬¦")
    
    assert avg_size <= 300, "å¹³å‡åˆ†å—å¤§å°åº”è¯¥åœ¨åˆç†èŒƒå›´å†…"
    
    print("âœ… åŸºäºæ ‡é¢˜çš„åˆ†å—æµ‹è¯•é€šè¿‡ï¼")


def test_real_document_chunking():
    """æµ‹è¯•çœŸå®æ–‡æ¡£çš„åˆ†å—æ•ˆæœ"""
    processor = MarkdownProcessor()
    
    # è¯»å–ä¸€ä¸ªçœŸå®çš„æ–‡æ¡£
    docs_path = Path("../docs/src/grammar/stmt.md")
    if not docs_path.exists():
        print("âš ï¸  çœŸå®æ–‡æ¡£ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    with open(docs_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æµ‹è¯•åˆ†å—
    chunks = processor.split_into_chunks(content)
    
    print("ğŸ“„ çœŸå®æ–‡æ¡£åˆ†å—æµ‹è¯•:")
    print(f"   æ–‡æ¡£: {docs_path.name}")
    print(f"   åŸå§‹é•¿åº¦: {len(content)} å­—ç¬¦")
    print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
    print()
    
    # åˆ†æåˆ†å—
    header_chunks = 0
    total_headers = 0
    
    for i, chunk in enumerate(chunks[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªåˆ†å—
        headers_in_chunk = chunk.count('#')
        total_headers += headers_in_chunk
        
        if headers_in_chunk > 0:
            header_chunks += 1
        
        print(f"   åˆ†å— {i}:")
        print(f"     é•¿åº¦: {len(chunk)} å­—ç¬¦")
        print(f"     æ ‡é¢˜æ•°é‡: {headers_in_chunk}")
        
        # æå–ç¬¬ä¸€ä¸ªæ ‡é¢˜
        lines = chunk.split('\n')
        for line in lines:
            if line.strip().startswith('#'):
                print(f"     ç¬¬ä¸€ä¸ªæ ‡é¢˜: {line.strip()}")
                break
        print()
    
    print(f"   åŒ…å«æ ‡é¢˜çš„åˆ†å—: {header_chunks}/{len(chunks)}")
    print(f"   æ€»æ ‡é¢˜æ•°: {total_headers}")
    
    # éªŒè¯åˆ†å—è´¨é‡
    assert len(chunks) > 0, "åº”è¯¥äº§ç”Ÿåˆ†å—"
    assert header_chunks > 0, "åº”è¯¥æœ‰åŒ…å«æ ‡é¢˜çš„åˆ†å—"
    
    print("âœ… çœŸå®æ–‡æ¡£åˆ†å—æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_header_based_chunking()
    test_real_document_chunking() 