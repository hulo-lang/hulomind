import os
from pathlib import Path
from src.processors.document_loader import DocumentLoader
from src.config.settings import settings


def test_chunking_effectiveness():
    """æµ‹è¯•åˆ†å—æ•ˆæœ"""
    loader = DocumentLoader()
    
    # åŠ è½½æ–‡æ¡£
    docs = loader.load_documents(max_workers=2)
    assert len(docs) > 0, "åº”è¯¥èƒ½åŠ è½½åˆ°æ–‡æ¡£"
    
    # åˆ›å»ºåˆ†å—
    chunks = loader.create_all_chunks(docs)
    assert len(chunks) > 0, "åº”è¯¥èƒ½åˆ›å»ºåˆ†å—"
    
    print(f"ğŸ“Š åˆ†å—ç»Ÿè®¡:")
    print(f"   åŸå§‹æ–‡æ¡£æ•°: {len(docs)}")
    print(f"   æ€»åˆ†å—æ•°: {len(chunks)}")
    print(f"   å¹³å‡æ¯æ–‡æ¡£åˆ†å—æ•°: {len(chunks) / len(docs):.1f}")
    
    # æ£€æŸ¥åˆ†å—å¤§å°
    chunk_sizes = [len(chunk.content) for chunk in chunks]
    avg_size = sum(chunk_sizes) / len(chunk_sizes)
    max_size = max(chunk_sizes)
    min_size = min(chunk_sizes)
    
    print(f"   åˆ†å—å¤§å°ç»Ÿè®¡:")
    print(f"     å¹³å‡: {avg_size:.0f} å­—ç¬¦")
    print(f"     æœ€å¤§: {max_size} å­—ç¬¦")
    print(f"     æœ€å°: {min_size} å­—ç¬¦")
    print(f"     é…ç½®çš„chunk_size: {settings.chunk_size}")
    
    # éªŒè¯åˆ†å—è´¨é‡
    assert avg_size <= settings.chunk_size * 1.2, "å¹³å‡åˆ†å—å¤§å°åº”è¯¥åœ¨åˆç†èŒƒå›´å†…"
    assert max_size <= settings.chunk_size * 1.5, "æœ€å¤§åˆ†å—ä¸åº”è¯¥è¿‡å¤§"
    
    # æ£€æŸ¥è¯­è¨€åˆ†å¸ƒ
    en_chunks = [c for c in chunks if c.language == "en"]
    zh_chunks = [c for c in chunks if c.language == "zh"]
    
    print(f"   è¯­è¨€åˆ†å¸ƒ:")
    print(f"     è‹±æ–‡åˆ†å—: {len(en_chunks)}")
    print(f"     ä¸­æ–‡åˆ†å—: {len(zh_chunks)}")
    
    # æ£€æŸ¥åˆ†ç±»åˆ†å¸ƒ
    categories = {}
    for chunk in chunks:
        cat = chunk.category
        categories[cat] = categories.get(cat, 0) + 1
    
    print(f"   åˆ†ç±»åˆ†å¸ƒ:")
    for cat, count in sorted(categories.items()):
        print(f"     {cat}: {count} ä¸ªåˆ†å—")
    
    print("âœ… åˆ†å—æµ‹è¯•é€šè¿‡ï¼")


def test_chunk_content_quality():
    """æµ‹è¯•åˆ†å—å†…å®¹è´¨é‡"""
    loader = DocumentLoader()
    docs = loader.load_documents(max_workers=2)
    chunks = loader.create_all_chunks(docs)
    
    # æ£€æŸ¥åˆ†å—å†…å®¹è´¨é‡
    for chunk in chunks[:5]:  # æ£€æŸ¥å‰5ä¸ªåˆ†å—
        assert len(chunk.content.strip()) > 0, "åˆ†å—å†…å®¹ä¸åº”ä¸ºç©º"
        assert not chunk.content.startswith("---"), "åˆ†å—ä¸åº”ä»¥frontmatterå¼€å§‹"
        assert chunk.chunk_index >= 0, "åˆ†å—ç´¢å¼•åº”è¯¥æœ‰æ•ˆ"
        assert chunk.start_pos < chunk.end_pos, "åˆ†å—ä½ç½®åº”è¯¥æœ‰æ•ˆ"
    
    print("âœ… åˆ†å—å†…å®¹è´¨é‡æµ‹è¯•é€šè¿‡ï¼")


if __name__ == "__main__":
    test_chunking_effectiveness()
    test_chunk_content_quality() 